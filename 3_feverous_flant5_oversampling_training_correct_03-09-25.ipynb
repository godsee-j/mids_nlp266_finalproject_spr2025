{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"BOVMc_2NpuPR"},"outputs":[],"source":["# Enhanced Flan-T5 Training for Misinformation Detection\n","# Optimized for M3 Max in notebook environment\n","# Import the dataset class from the separate file\n","from dataset_utils import FeverousDataset, create_balanced_dataset\n","import json\n","import os\n","import numpy as np\n","import random\n","from tqdm.notebook import tqdm\n","import matplotlib.pyplot as plt\n","from IPython.display import display, HTML\n","\n","import torch\n","from torch.utils.data import DataLoader\n","from transformers import (\n","    AutoModelForSeq2SeqLM,\n","    AutoTokenizer,\n","    Seq2SeqTrainingArguments,\n","    Seq2SeqTrainer,\n","    DataCollatorForSeq2Seq,\n",")\n","\n","# Define the oversample_minority_classes function directly in the script\n","def oversample_minority_classes(dataset, seed=42):\n","    \"\"\"\n","    Oversample minority classes to match the majority class\n","    while keeping all original data\n","    \"\"\"\n","    random.seed(seed)\n","    print(\"Oversampling minority classes...\")\n","\n","    # Group examples by verdict\n","    by_verdict = {\"SUPPORTS\": [], \"REFUTES\": [], \"NOT ENOUGH INFO\": []}\n","    for example in dataset.data:\n","        verdict = example[\"verdict\"]\n","        if verdict in by_verdict:\n","            by_verdict[verdict].append(example)\n","\n","    # Print class distribution before oversampling\n","    print(\"Class distribution before oversampling:\")\n","    for verdict, examples in by_verdict.items():\n","        print(f\"  {verdict}: {len(examples)} examples\")\n","\n","    # Find the majority class count\n","    max_count = max(len(examples) for examples in by_verdict.values())\n","\n","    # Oversample minority classes\n","    balanced = []\n","    for verdict, examples in by_verdict.items():\n","        if len(examples) < max_count:\n","            # Oversample with replacement to match the majority class\n","            additional = random.choices(examples, k=max_count-len(examples))\n","            balanced.extend(examples + additional)\n","        else:\n","            balanced.extend(examples)\n","\n","    # Shuffle the final balanced dataset\n","    random.shuffle(balanced)\n","\n","    # Update the dataset\n","    dataset.data = balanced\n","    print(f\"Created oversampled dataset with {len(balanced)} examples\")\n","\n","    return dataset\n","\n","# Check if MPS (Metal Performance Shaders) is available for M3 Max\n","device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n","print(f\"Using device: {device}\")\n","\n","# Define metrics computation\n","def compute_metrics(eval_preds):\n","    preds, labels = eval_preds\n","\n","    # Decode predictions and labels\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","    # Replace -100 with pad token id before decoding\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","\n","    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    # Normalize predictions\n","    normalized_preds = []\n","    for pred in decoded_preds:\n","        pred = pred.strip().upper()\n","        if \"SUPPORT\" in pred:\n","            normalized_preds.append(\"SUPPORTED\")\n","        elif \"REFUT\" in pred or \"FALSE\" in pred:\n","            normalized_preds.append(\"REFUTED\")\n","        elif \"NOT ENOUGH\" in pred or \"INSUFFICIENT\" in pred or \"UNKNOWN\" in pred:\n","            normalized_preds.append(\"NOT ENOUGH INFORMATION\")\n","        else:\n","            normalized_preds.append(pred)  # Keep as is if no match\n","\n","    # Normalize labels\n","    normalized_labels = [label.strip().upper() for label in decoded_labels]\n","\n","    # Calculate accuracy\n","    correct = sum(1 for p, l in zip(normalized_preds, normalized_labels) if p == l)\n","    accuracy = correct / len(normalized_preds)\n","\n","    # Calculate per-class metrics\n","    classes = [\"SUPPORTED\", \"REFUTED\", \"NOT ENOUGH INFORMATION\"]\n","    per_class_metrics = {}\n","\n","    for cls in classes:\n","        true_positives = sum(1 for p, l in zip(normalized_preds, normalized_labels) if p == cls and l == cls)\n","        false_positives = sum(1 for p, l in zip(normalized_preds, normalized_labels) if p == cls and l != cls)\n","        false_negatives = sum(1 for p, l in zip(normalized_preds, normalized_labels) if p != cls and l == cls)\n","\n","        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n","        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n","        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n","\n","        per_class_metrics[cls] = {\n","            \"precision\": precision,\n","            \"recall\": recall,\n","            \"f1\": f1\n","        }\n","\n","    # Calculate macro F1\n","    macro_f1 = sum(metrics[\"f1\"] for metrics in per_class_metrics.values()) / len(per_class_metrics)\n","\n","    return {\n","        \"accuracy\": accuracy,\n","        \"macro_f1\": macro_f1,\n","        **{f\"{cls.lower()}_{metric}\": value\n","           for cls, metrics in per_class_metrics.items()\n","           for metric, value in metrics.items()}\n","    }\n","\n","# Function to visualize training results\n","def plot_training_results(trainer_history):\n","    train_loss = trainer_history.state.log_history\n","\n","    # Extract training and evaluation metrics\n","    train_metrics = [x for x in train_loss if 'loss' in x and 'eval' not in x]\n","    eval_metrics = [x for x in train_loss if 'eval_loss' in x]\n","\n","    # Plot training loss\n","    plt.figure(figsize=(15, 5))\n","    plt.subplot(1, 2, 1)\n","    plt.plot([x['step'] for x in train_metrics], [x['loss'] for x in train_metrics])\n","    plt.title('Training Loss')\n","    plt.xlabel('Step')\n","    plt.ylabel('Loss')\n","\n","    # Plot evaluation metrics\n","    plt.subplot(1, 2, 2)\n","    plt.plot([x['step'] for x in eval_metrics], [x['eval_accuracy'] for x in eval_metrics], label='Accuracy')\n","    plt.plot([x['step'] for x in eval_metrics], [x['eval_macro_f1'] for x in eval_metrics], label='Macro F1')\n","    plt.title('Evaluation Metrics')\n","    plt.xlabel('Step')\n","    plt.ylabel('Score')\n","    plt.legend()\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","# Configure training parameters\n","model_name = \"google/flan-t5-small\"  # Options: google/flan-t5-small, google/flan-t5-base, google/flan-t5-large\n","data_dir = \"feverous_prepared\"  # Directory containing the prepared data\n","output_dir = \"flan-t5-feverous\"  # Output directory for model and checkpoints\n","max_length = 512  # Maximum sequence length\n","\n","# Training hyperparameters optimized for M3 Max\n","batch_size = 16  # Adjust based on model size (16 for small, 8 for base, 4 for large)\n","gradient_accumulation_steps = 1  # Increase to 2 or 4 for larger models\n","learning_rate = 5e-5  # Learning rate\n","weight_decay = 0.01  # Weight decay for regularization\n","num_epochs = 3  # Number of training epochs\n","warmup_ratio = 0.1  # Ratio of total training steps to use for warmup\n","\n","# Dataset options\n","max_train_samples = -1  # Use -1 for full dataset, or a smaller number for testing\n","max_eval_samples = -1  # Use -1 for full dataset, or a smaller number for testing\n","balance_dataset = True  # Whether to balance the dataset\n","max_per_class = 10000  # Maximum examples per class when balancing\n","\n","# Performance and optimization\n","use_fp16 = False  # MPS on M3 Max doesn't support fp16 training\n","gradient_checkpointing = False  # Enable for larger models to save memory\n","num_workers = 4  # Number of workers for data loading\n","logging_steps = 100  # Number of steps between logging updates\n","seed = 42  # Random seed for reproducibility\n","\n","# Set random seeds for reproducibility\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","\n","# Create output directory\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# Load tokenizer and model\n","print(f\"Loading model: {model_name}\")\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n","\n","# Enable gradient checkpointing to save memory\n","if gradient_checkpointing:\n","    print(\"Enabling gradient checkpointing\")\n","    model.gradient_checkpointing_enable()\n","\n","# Prepare datasets\n","print(f\"Loading datasets from {data_dir}\")\n","train_dataset = FeverousDataset(\n","    os.path.join(data_dir, \"feverous_train_prompts.json\"),\n","    tokenizer,\n","    max_length=max_length\n",")\n","\n","eval_dataset = FeverousDataset(\n","    os.path.join(data_dir, \"feverous_dev_prompts.json\"),\n","    tokenizer,\n","    max_length=max_length\n",")\n","\n","print(f\"Train dataset size: {len(train_dataset)}\")\n","print(f\"Eval dataset size: {len(eval_dataset)}\")\n","\n","# If using a subset for faster training/testing\n","if max_train_samples > 0:\n","    train_dataset.data = train_dataset.data[:max_train_samples]\n","    print(f\"Using {len(train_dataset)} training examples\")\n","\n","if max_eval_samples > 0:\n","    eval_dataset.data = eval_dataset.data[:max_eval_samples]\n","    print(f\"Using {len(eval_dataset)} evaluation examples\")\n","\n","# Apply oversampling to balance the dataset if requested\n","if balance_dataset:\n","    train_dataset = oversample_minority_classes(\n","        train_dataset,\n","        seed=seed\n","    )\n","\n","# Data collator\n","data_collator = DataCollatorForSeq2Seq(\n","    tokenizer=tokenizer,\n","    model=model,\n","    padding=\"max_length\",\n","    max_length=max_length\n",")\n","\n","# Calculate training steps for warmup\n","num_update_steps_per_epoch = len(train_dataset) // (batch_size * gradient_accumulation_steps)\n","max_train_steps = num_epochs * num_update_steps_per_epoch\n","\n","# Calculate warmup steps\n","warmup_steps = int(warmup_ratio * max_train_steps)\n","print(f\"Warmup steps: {warmup_steps} of {max_train_steps} total steps\")\n","\n","# Training arguments\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=output_dir,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    learning_rate=learning_rate,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    gradient_accumulation_steps=gradient_accumulation_steps,\n","    weight_decay=weight_decay,\n","    save_total_limit=3,\n","    num_train_epochs=num_epochs,\n","    predict_with_generate=True,\n","    generation_max_length=10,\n","    fp16=use_fp16,  # MPS doesn't support fp16\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"macro_f1\",\n","    greater_is_better=True,\n","    report_to=\"tensorboard\",\n","    warmup_steps=warmup_steps,\n","    logging_steps=logging_steps,\n","    logging_dir=os.path.join(output_dir, \"logs\"),\n","    seed=seed,\n","    dataloader_num_workers=num_workers,  # We can keep multiple workers now\n",")\n","\n","# Initialize Trainer\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")\n","\n","# Train the model\n","print(\"Starting training...\")\n","train_result = trainer.train()\n","\n","# Plot training results\n","plot_training_results(trainer)\n","\n","# Save the final model\n","trainer.save_model(os.path.join(output_dir, \"final_model\"))\n","tokenizer.save_pretrained(os.path.join(output_dir, \"final_model\"))\n","\n","# Evaluate the model\n","print(\"Evaluating final model...\")\n","results = trainer.evaluate()\n","print(results)\n","\n","# Save evaluation results\n","with open(os.path.join(output_dir, \"eval_results.json\"), \"w\") as f:\n","    json.dump(results, f, indent=4)\n","\n","# Display a table of evaluation results\n","display(HTML(\"<h3>Evaluation Results</h3>\"))\n","results_table = \"<table><tr><th>Metric</th><th>Value</th></tr>\"\n","for metric, value in results.items():\n","    results_table += f\"<tr><td>{metric}</td><td>{value:.4f}</td></tr>\"\n","results_table += \"</table>\"\n","display(HTML(results_table))\n","\n","# Inference function for testing the model\n","def predict_verdict(claim, evidence, model_path=os.path.join(output_dir, \"final_model\")):\n","    # Load the fine-tuned model and tokenizer\n","    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n","    tokenizer = AutoTokenizer.from_pretrained(model_path)\n","\n","    # Create the prompt\n","    prompt = f\"Claim: {claim}\\n\\nEvidence: {evidence}\\n\\nIs the claim supported, refuted, or is there not enough information?\"\n","\n","    # Tokenize the prompt\n","    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=max_length, truncation=True)\n","\n","    # Generate prediction\n","    outputs = model.generate(\n","        inputs.input_ids,\n","        max_length=10,\n","        num_beams=4,\n","        early_stopping=True\n","    )\n","\n","    # Decode the prediction\n","    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","    # Normalize the prediction\n","    prediction = prediction.strip().upper()\n","    if \"SUPPORT\" in prediction:\n","        normalized_pred = \"SUPPORTED\"\n","    elif \"REFUT\" in prediction or \"FALSE\" in prediction:\n","        normalized_pred = \"REFUTED\"\n","    elif \"NOT ENOUGH\" in prediction or \"INSUFFICIENT\" in prediction or \"UNKNOWN\" in prediction:\n","        normalized_pred = \"NOT ENOUGH INFORMATION\"\n","    else:\n","        normalized_pred = prediction\n","\n","    return normalized_pred\n","\n","# Example usage:\n","# prediction = predict_verdict(\n","#     \"Paris is the capital of France.\",\n","#     \"Paris is the capital and most populous city of France.\"\n","# )\n","# print(f\"Prediction: {prediction}\")"]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}