{"cells":[{"cell_type":"markdown","metadata":{"id":"-tJNWbs7l2Zq"},"source":["How we processed claims to make train_all_processed.json and dev_all_process.json\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mx1Czo3fl2Zu"},"outputs":[],"source":["import json\n","import os\n","from tqdm import tqdm\n","from feverous.database.feverous_db import FeverousDB\n","from feverous.utils.wiki_page import WikiPage\n","from feverous.utils.annotation_processor import AnnotationProcessor\n","\n","class FeverousVerifier:\n","    def __init__(self, db_path):\n","        \"\"\"Initialize the FEVEROUS verifier with database path.\"\"\"\n","        print(\"Initializing FEVEROUS database...\")\n","        self.db = FeverousDB(db_path)\n","        self.results = {\"SUPPORTS\": 0, \"REFUTES\": 0, \"NOT ENOUGH INFO\": 0}\n","        self.processed_claims = []\n","        self.page_cache = {}\n","\n","    def load_dataset(self, input_path):\n","        \"\"\"Load dataset with optimized evidence extraction.\"\"\"\n","        print(f\"Loading data from {input_path}\")\n","        annotations = list(AnnotationProcessor(input_path))\n","        print(f\"Loaded {len(annotations)} examples from {input_path}\")\n","        return annotations\n","\n","    def get_wiki_page(self, page_name):\n","        \"\"\"Retrieve Wikipedia page with caching for efficiency.\"\"\"\n","        if page_name in self.page_cache:\n","            return self.page_cache[page_name]\n","\n","        page_json = self.db.get_doc_json(page_name)\n","        if not page_json:\n","            return None\n","\n","        wiki_page = WikiPage(page_name, page_json)\n","        self.page_cache[page_name] = wiki_page\n","        return wiki_page\n","\n","    def process_claims(self, annotations, batch_size=5000, output_prefix=\"batch\", final_json_file=None):\n","        \"\"\"Process claims in batches, saving results for each batch and a final JSON file.\"\"\"\n","        total = len(annotations)\n","        print(f\"Processing {total} annotations in batches of {batch_size}\")\n","\n","        batch_claims = []\n","        batch_results = {\"SUPPORTS\": 0, \"REFUTES\": 0, \"NOT ENOUGH INFO\": 0}\n","        total_results = {\"SUPPORTS\": 0, \"REFUTES\": 0, \"NOT ENOUGH INFO\": 0}\n","\n","        # Store all processed claims\n","        all_claims = [] if final_json_file else None\n","\n","        for i, annotation in enumerate(tqdm(annotations, desc=\"Processing FEVEROUS data\")):\n","            result, evidence = self.verify_claim(annotation)\n","\n","            # Update batch and total statistics\n","            batch_results[result] = batch_results.get(result, 0) + 1\n","            total_results[result] = total_results.get(result, 0) + 1\n","\n","            # Create claim object\n","            claim_obj = {\n","                \"id\": annotation.id,\n","                \"claim\": annotation.claim,\n","                \"verdict\": annotation.verdict,\n","                \"verification_result\": result,\n","                \"evidence\": evidence\n","            }\n","\n","            # Add to batch claims\n","            batch_claims.append(claim_obj)\n","\n","            # Store for final JSON file\n","            if all_claims is not None:\n","                # Make a serializable copy to ensure it can be saved later\n","                serializable_claim = self._make_serializable(claim_obj)\n","                all_claims.append(serializable_claim)\n","\n","            # Save batch when it reaches the batch size or at the end\n","            if (i + 1) % batch_size == 0 or i == total - 1:\n","                batch_num = (i // batch_size) + 1\n","                start_idx = ((batch_num - 1) * batch_size) + 1\n","                end_idx = min(batch_num * batch_size, total)\n","\n","                batch_file = f\"{output_prefix}_{start_idx}_to_{end_idx}.json\"\n","\n","                # Save the batch\n","                self.save_batch(batch_claims, batch_results, batch_file)\n","\n","                # Reset for next batch\n","                batch_claims = []\n","                batch_results = {\"SUPPORTS\": 0, \"REFUTES\": 0, \"NOT ENOUGH INFO\": 0}\n","\n","                # Clear cache between batches\n","                self.page_cache = {}\n","                print(f\"Completed batch {batch_num}: saved examples {start_idx}-{end_idx} to {batch_file}\")\n","\n","            # Clear cache periodically for memory management within a batch\n","            elif (i + 1) % 1000 == 0:\n","                self.page_cache = {}\n","\n","        # Save the final JSON file with all claims\n","        if final_json_file and all_claims:\n","            print(f\"Saving all {len(all_claims)} processed claims to {final_json_file}...\")\n","            with open(final_json_file, 'w') as f:\n","                json.dump({\n","                    \"statistics\": total_results,\n","                    \"processed_claims\": all_claims\n","                }, f, indent=2)\n","            print(f\"ðŸ“„ All claims saved to {final_json_file}\")\n","\n","        # Create a summary file with overall statistics\n","        summary_file = f\"{output_prefix}_summary.json\"\n","        with open(summary_file, 'w') as f:\n","            json.dump({\n","                \"total_processed\": total,\n","                \"total_statistics\": total_results\n","            }, f, indent=2)\n","        print(f\"Summary saved to {summary_file}\")\n","\n","        return total_results\n","\n","    def _make_serializable(self, claim_obj):\n","        \"\"\"Make a claim object fully serializable.\"\"\"\n","        serializable_claim = {\n","            \"id\": claim_obj[\"id\"],\n","            \"claim\": claim_obj[\"claim\"],\n","            \"verdict\": claim_obj[\"verdict\"],\n","            \"verification_result\": claim_obj[\"verification_result\"],\n","            \"evidence\": []\n","        }\n","\n","        # Convert evidence to serializable format\n","        for evidence_set in claim_obj[\"evidence\"]:\n","            serializable_set = []\n","            for ev in evidence_set:\n","                serializable_ev = {\n","                    \"id\": ev[\"id\"],\n","                    \"content\": str(ev[\"content\"]),  # Ensure content is a string\n","                }\n","                if \"error\" in ev:\n","                    serializable_ev[\"error\"] = ev[\"error\"]\n","                serializable_set.append(serializable_ev)\n","            serializable_claim[\"evidence\"].append(serializable_set)\n","\n","        return serializable_claim\n","\n","    def verify_claim(self, annotation):\n","        \"\"\"Verify a claim by extracting only relevant evidence.\"\"\"\n","        claim_id = getattr(annotation, \"id\", \"unknown\")\n","\n","        if not annotation.evidence:\n","            return \"NOT ENOUGH INFO\", []\n","\n","        # Group evidence IDs by page to minimize database lookups\n","        evidence_by_page = {}\n","        for evidence_set in annotation.evidence:\n","            for ev_id in evidence_set:\n","                page_name = ev_id.split(\"_\")[0]\n","                if page_name not in evidence_by_page:\n","                    evidence_by_page[page_name] = []\n","                evidence_by_page[page_name].append(ev_id)\n","\n","        # Extract all relevant evidence efficiently\n","        all_evidence = []\n","        missing_evidence = []\n","\n","        for page_name, ev_ids in evidence_by_page.items():\n","            wiki_page = self.get_wiki_page(page_name)\n","            if not wiki_page:\n","                missing_evidence.extend(ev_ids)\n","                print(f\"Warning: Page '{page_name}' not found for claim {claim_id}\")\n","                continue\n","\n","            # Extract evidence for this page in a single pass\n","            try:\n","                page_evidence = self.extract_page_evidence(wiki_page, ev_ids)\n","                all_evidence.extend(page_evidence)\n","            except Exception as e:\n","                print(f\"Error processing evidence for claim {claim_id}: {str(e)}\")\n","                missing_evidence.extend(ev_ids)\n","\n","        # Organize evidence by evidence sets as they appear in the annotation\n","        organized_evidence = []\n","        for evidence_set in annotation.evidence:\n","            set_evidence = []\n","            for ev_id in evidence_set:\n","                # Find this evidence in our extracted evidence\n","                found = False\n","                for ev in all_evidence:\n","                    if ev[\"id\"] == ev_id:\n","                        set_evidence.append(ev)\n","                        found = True\n","                        break\n","\n","                if not found:\n","                    set_evidence.append({\"id\": ev_id, \"content\": \"Evidence not found\", \"error\": True})\n","\n","            organized_evidence.append(set_evidence)\n","\n","        # Return both the verdict and the organized evidence\n","        return annotation.verdict, organized_evidence\n","\n","    def extract_page_evidence(self, wiki_page, evidence_ids):\n","        \"\"\"Extract all evidence from a single page in an optimized way.\"\"\"\n","        result = []\n","\n","        # Pre-extract all data structures to avoid repeated calls\n","        sentences = wiki_page.get_sentences()\n","        tables = wiki_page.get_tables()\n","        lists = wiki_page.get_lists()\n","\n","        # Process each evidence ID\n","        for ev_id in evidence_ids:\n","            try:\n","                page_name = ev_id.split(\"_\")[0]\n","\n","                if \"_sentence_\" in ev_id:\n","                    # Extract sentence evidence\n","                    sentence_id = int(ev_id.split(\"_sentence_\")[1])\n","                    if sentence_id < len(sentences):\n","                        content = str(sentences[sentence_id])\n","                    else:\n","                        content = f\"Error: Sentence index {sentence_id} out of range\"\n","\n","                elif \"_cell_\" in ev_id:\n","                    # Extract table cell evidence\n","                    cell_parts = ev_id.split(\"_cell_\")[1].split(\"_\")\n","                    table_idx, row_idx, cell_idx = map(int, cell_parts)\n","                    if table_idx < len(tables):\n","                        rows = tables[table_idx].get_rows()\n","                        if row_idx < len(rows):\n","                            cells = rows[row_idx].get_row_cells()\n","                            if cell_idx < len(cells):\n","                                # Convert Cell object to string representation\n","                                cell = cells[cell_idx]\n","                                if hasattr(cell, 'get_text'):\n","                                    content = cell.get_text()\n","                                else:\n","                                    content = str(cell)\n","                            else:\n","                                content = f\"Error: Cell index {cell_idx} out of range\"\n","                        else:\n","                            content = f\"Error: Row index {row_idx} out of range\"\n","                    else:\n","                        content = f\"Error: Table index {table_idx} out of range\"\n","\n","                elif \"_item_\" in ev_id:\n","                    # Extract list item evidence\n","                    item_parts = ev_id.split(\"_item_\")[1].split(\"_\")\n","                    list_idx, item_idx = map(int, item_parts)\n","                    if list_idx < len(lists):\n","                        list_items = lists[list_idx].get_list_by_level(0)\n","                        if item_idx < len(list_items):\n","                            content = str(list_items[item_idx])\n","                        else:\n","                            content = f\"Error: Item index {item_idx} out of range\"\n","                    else:\n","                        content = f\"Error: List index {list_idx} out of range\"\n","\n","                else:\n","                    content = f\"Unknown evidence type: {ev_id}\"\n","\n","                result.append({\"id\": ev_id, \"content\": content})\n","\n","            except Exception as e:\n","                print(f\"Error processing evidence {ev_id}: {str(e)}\")\n","                result.append({\"id\": ev_id, \"content\": f\"Error: {str(e)}\", \"error\": True})\n","\n","        return result\n","\n","    def save_batch(self, claims, results, output_file):\n","        \"\"\"Save a batch of results with JSON serialization handling.\"\"\"\n","        try:\n","            with open(output_file, 'w') as f:\n","                json.dump({\n","                    \"statistics\": results,\n","                    \"processed_claims\": claims\n","                }, f, indent=2)\n","            print(f\"Batch saved to {output_file}\")\n","        except TypeError as e:\n","            print(f\"Error during JSON serialization: {str(e)}\")\n","            print(\"Attempting to fix non-serializable objects...\")\n","\n","            # Create a serializable copy of claims\n","            serializable_claims = []\n","            for claim in claims:\n","                serializable_claim = self._make_serializable(claim)\n","                serializable_claims.append(serializable_claim)\n","\n","            # Try saving again with the serializable copy\n","            with open(output_file, 'w') as f:\n","                json.dump({\n","                    \"statistics\": results,\n","                    \"processed_claims\": serializable_claims\n","                }, f, indent=2)\n","            print(f\"Batch saved to {output_file} after fixing serialization issues\")\n","\n","# Main Execution\n","if __name__ == \"__main__\":\n","    data_dir = os.path.expanduser(\"~/Documents/data\")\n","    db_file = os.path.join(data_dir, \"feverous_wikiv1.db\")\n","    train_file = os.path.join(data_dir, \"feverous_train_challenges.jsonl\")\n","    dev_file = os.path.join(data_dir, \"feverous_dev_challenges.jsonl\")\n","\n","    verifier = FeverousVerifier(db_path=db_file)\n","\n","    # Process train data in batches of 5000 and create final JSON\n","    print(\"\\n Processing TRAIN dataset\")\n","    train_annotations = verifier.load_dataset(train_file)\n","    train_results = verifier.process_claims(\n","        train_annotations,\n","        batch_size=10000,\n","        output_prefix=\"train_batch\",\n","        final_json_file=\"train_all_processed.json\"\n","    )\n","\n","    # Process dev data in batches of 5000 and create final JSON\n","    print(\"\\n Processing DEV dataset\")\n","    dev_annotations = verifier.load_dataset(dev_file)\n","    dev_results = verifier.process_claims(\n","        dev_annotations,\n","        batch_size=5000,\n","        output_prefix=\"dev_batch\",\n","        final_json_file=\"dev_all_processed.json\"\n","    )\n","\n","    # Create a combined summary of both train and dev\n","    print(\"\\n Creating combined summary of train and dev data\")\n","    combined_stats = {\n","        \"train\": train_results,\n","        \"dev\": dev_results,\n","        \"total\": {k: train_results.get(k, 0) + dev_results.get(k, 0) for k in set(train_results) | set(dev_results)}\n","    }\n","\n","    with open(\"feverous_combined_summary.json\", 'w') as f:\n","        json.dump({\n","            \"statistics\": combined_stats,\n","            \"total_train_examples\": len(train_annotations),\n","            \"total_dev_examples\": len(dev_annotations)\n","        }, f, indent=2)\n","\n","    print(\"\\n All processing complete!\")\n","    print(f\"Train statistics: {train_results}\")\n","    print(f\"Dev statistics: {dev_results}\")\n","    print(f\"Combined statistics: {combined_stats['total']}\")\n","    print(f\" Train data saved to: train_all_processed.json\")\n","    print(f\" Dev data saved to: dev_all_processed.json\")"]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}